{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30408fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, numpy as np, matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "CAMERA_INDEX = 0\n",
    "FRAME_WIDTH, FRAME_HEIGHT = 1280, 720\n",
    "\n",
    "def _gamma(img, g=1.2):\n",
    "    lut = np.array([((i/255.0)**(1.0/g))*255 for i in range(256)]).astype(np.uint8)\n",
    "    return cv2.LUT(img, lut)\n",
    "\n",
    "haar = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "def haar_detect_boxes_rgb(rgb):\n",
    "    H,W = rgb.shape[:2]\n",
    "    SCALE = 0.6  # 0.4ã€œ0.8ã§èª¿æ•´å¯\n",
    "    small = cv2.resize(rgb, (int(W*SCALE), int(H*SCALE)), interpolation=cv2.INTER_AREA)\n",
    "    gray  = cv2.cvtColor(small, cv2.COLOR_RGB2GRAY)\n",
    "    gray  = cv2.equalizeHist(gray)\n",
    "    gray  = _gamma(gray, g=1.1)\n",
    "    faces = haar.detectMultiScale(gray, scaleFactor=1.05, minNeighbors=4, minSize=(60,60))\n",
    "    boxes = []\n",
    "    for (x,y,w,h) in faces:\n",
    "        l = int(x / SCALE); t = int(y / SCALE)\n",
    "        r = int((x+w) / SCALE); b = int((y+h) / SCALE)\n",
    "        boxes.append((t,r,b,l))\n",
    "    return boxes\n",
    "\n",
    "def hog_detect_boxes_rgb(rgb, up=1):\n",
    "    import face_recognition\n",
    "    return face_recognition.face_locations(rgb, number_of_times_to_upsample=up, model=\"hog\")\n",
    "\n",
    "# æ’®å½±ã—ã¦ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼\n",
    "cap = cv2.VideoCapture(CAMERA_INDEX, cv2.CAP_AVFOUNDATION)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, FRAME_WIDTH)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)\n",
    "\n",
    "# éœ²å‡ºå®‰å®šã®ãŸã‚æ•°ãƒ•ãƒ¬ãƒ¼ãƒ æ¨ã¦ã‚‹\n",
    "for _ in range(6):\n",
    "    cap.read()\n",
    "\n",
    "ok, frame = cap.read()\n",
    "cap.release()\n",
    "assert ok, \"ã‚«ãƒ¡ãƒ©ã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ\"\n",
    "\n",
    "rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "vis = rgb.copy()\n",
    "\n",
    "boxes = haar_detect_boxes_rgb(rgb)\n",
    "fallback = False\n",
    "if not boxes:\n",
    "    # HOGã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆé‡ã„ãŒè¦‹ã¤ã‹ã‚Šã‚„ã™ã„ï¼‰\n",
    "    try:\n",
    "        boxes = hog_detect_boxes_rgb(rgb, up=1)\n",
    "        fallback = True\n",
    "    except Exception as e:\n",
    "        print(\"HOG fallback error:\", e)\n",
    "\n",
    "for (t,r,b,l) in boxes:\n",
    "    cv2.rectangle(vis, (l,t), (r,b), (0,255,0), 2)\n",
    "\n",
    "clear_output(wait=True)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.imshow(vis); plt.axis(\"off\")\n",
    "plt.title(f\"æ¤œå‡ºæ•°: {len(boxes)}  ({'HOG' if fallback else 'Haar'})\")\n",
    "plt.show()\n",
    "\n",
    "print(\"æ¤œå‡ºãƒœãƒƒã‚¯ã‚¹:\", boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57296cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, uuid\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import cv2, numpy as np, face_recognition\n",
    "\n",
    "PEOPLE_DIR = Path(\"face_data/people\"); PEOPLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CAMERA_INDEX = 0\n",
    "FRAME_WIDTH, FRAME_HEIGHT = 1280, 720\n",
    "\n",
    "haar = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "def _haar_boxes(rgb):\n",
    "    H,W = rgb.shape[:2]\n",
    "    SCALE = 0.6\n",
    "    small = cv2.resize(rgb, (int(W*SCALE), int(H*SCALE)), interpolation=cv2.INTER_AREA)\n",
    "    gray  = cv2.cvtColor(small, cv2.COLOR_RGB2GRAY)\n",
    "    gray  = cv2.equalizeHist(gray)\n",
    "    faces = haar.detectMultiScale(gray, scaleFactor=1.05, minNeighbors=4, minSize=(60,60))\n",
    "    boxes = []\n",
    "    for (x,y,w,h) in faces:\n",
    "        l = int(x / SCALE); t = int(y / SCALE)\n",
    "        r = int((x+w) / SCALE); b = int((y+h) / SCALE)\n",
    "        boxes.append((t,r,b,l))\n",
    "    return boxes\n",
    "\n",
    "def _hog_boxes(rgb, up=1):\n",
    "    return face_recognition.face_locations(rgb, number_of_times_to_upsample=up, model=\"hog\")\n",
    "\n",
    "def detect_boxes_robust(rgb):\n",
    "    boxes = _haar_boxes(rgb)\n",
    "    if boxes:\n",
    "        return boxes, \"Haar\"\n",
    "    boxes = _hog_boxes(rgb, up=1)\n",
    "    if boxes:\n",
    "        return boxes, \"HOGx1\"\n",
    "    boxes = _hog_boxes(rgb, up=2)\n",
    "    return boxes, \"HOGx2\" if boxes else ([], \"None\")\n",
    "\n",
    "def register_person_robust(name: str, meta: dict, shots: int = 12, interval: float = 0.2):\n",
    "    cap = cv2.VideoCapture(CAMERA_INDEX, cv2.CAP_AVFOUNDATION)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, FRAME_WIDTH)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)\n",
    "\n",
    "    encs, last_crop, used = [], None, None\n",
    "\n",
    "    # éœ²å‡ºå®‰å®š\n",
    "    for _ in range(6):\n",
    "        cap.read()\n",
    "\n",
    "    for i in range(shots):\n",
    "        ok, frame = cap.read()\n",
    "        if not ok: break\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        boxes, used = detect_boxes_robust(rgb)\n",
    "        if boxes:\n",
    "            t,r,b,l = max(boxes, key=lambda x:(x[2]-x[0])*(x[1]-x[3]))\n",
    "            vecs = face_recognition.face_encodings(rgb, known_face_locations=[(t,r,b,l)], num_jitters=1)\n",
    "            if vecs:\n",
    "                encs.append(vecs[0].astype(np.float64))\n",
    "                last_crop = frame[t:b, l:r]\n",
    "        cv2.waitKey(int(interval*1000))\n",
    "    cap.release()\n",
    "\n",
    "    if not encs:\n",
    "        print(\"âš ï¸ é¡”ãŒæ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æ˜ã‚‹ã•/è·é›¢/æ­£é¢ã‚’èª¿æ•´ã—ã¦å†è©¦è¡Œã€‚ï¼ˆæ¤œå‡ºå™¨:\", used, \"ï¼‰\")\n",
    "        return\n",
    "\n",
    "    pid = uuid.uuid4().hex[:12]\n",
    "    pdir = PEOPLE_DIR / f\"{pid}_{name}\"\n",
    "    pdir.mkdir(parents=True, exist_ok=True)\n",
    "    np.save(str(pdir/\"encodings.npy\"), np.vstack(encs))\n",
    "    (pdir/\"faces\").mkdir(parents=True, exist_ok=True)\n",
    "    if last_crop is not None:\n",
    "        now_ts = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "        cv2.imwrite(str(pdir/\"faces\"/f\"{now_ts}.jpg\"), last_crop)\n",
    "    meta_doc = {\"name\": name, \"meta\": meta, \"created_at\": datetime.utcnow().isoformat()}\n",
    "    (pdir/\"meta.json\").write_text(json.dumps(meta_doc, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"âœ… ç™»éŒ²å®Œäº†: {name}  shots={len(encs)}  detector={used}  dir={pdir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c68496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_person_robust(\n",
    "    \"ishioka\",\n",
    "    {\n",
    "        \"å‡ºèº«\": \"æœ­å¹Œ\",\n",
    "        \"å­¦å¹´\": \"M1\",\n",
    "        \"æ‰€å±\": \"UTokyo\",\n",
    "        \"ç ”ç©¶ãƒ†ãƒ¼ãƒ\": \"æ´¥æ³¢äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ãƒ»åå°„æ³¢è§£æ\",\n",
    "        \"è©±ã—ãŸå†…å®¹\": \"æ´¥æ³¢ã®ARå¯è¦–åŒ–ã‚¢ãƒ—ãƒªé–‹ç™ºã«ã¤ã„ã¦è­°è«–ã€‚Kamchatka 2025æƒ³å®šã®æ³¢æºãƒ¢ãƒ‡ãƒ«ç²¾åº¦ã®è©±ã‚‚ã€‚\",\n",
    "        \"ç¬¬ä¸€å°è±¡\": \"æ˜ã‚‹ãã¦è©±ã—ã‚„ã™ãã€æŠ€è¡“ã¸ã®èˆˆå‘³ãŒå¼·ã„ã€‚\",\n",
    "        \"è¦šãˆã¦ãŠããŸã„ã“ã¨\": [\n",
    "            \"Python ã¨ iOS ã®ä¸¡æ–¹ã§ãã‚‹\",\n",
    "            \"åœ°éœ‡ç ”æ‰€å±\",\n",
    "            \"é¡”èªè­˜ã‚¢ãƒ—ãƒªã«ã‚‚èˆˆå‘³ã‚ã‚Š\"\n",
    "        ]\n",
    "    },\n",
    "    shots=12,\n",
    "    interval=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd79af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FaceID: 5ç§’ã‚­ãƒ£ãƒ—ãƒãƒ£ â†’ èªè­˜ â†’ ç”»åƒ/éŸ³å£°ä»˜ãã§Mail.appè‡ªå‹•é€ä¿¡ï¼ˆã‚·ãƒ³ãƒ—ãƒ«TTSï¼‰ ===\n",
    "# ä¾å­˜: opencv-python, face_recognition, numpy, pillow, macOS ã® say ã‚³ãƒãƒ³ãƒ‰ (æ¨™æº–)\n",
    "import os, json, subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np, cv2, face_recognition\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# ====== è¨­å®š ======\n",
    "RECIPIENT = \"6301043919@utac.u-tokyo.ac.jp\"   # â†é€ã‚Šå…ˆã‚’å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´\n",
    "CAMERA_INDEX = 0                               # å†…è”µ=0, å¤–ä»˜=1 ãªã©\n",
    "FRAME_WIDTH, FRAME_HEIGHT = 1280, 720\n",
    "FACE_TOLERANCE = 0.55                          # 0.47ã€œ0.55ï¼ˆä½ã„ã»ã©å³å¯†ï¼‰\n",
    "\n",
    "# ====== æ—¥æœ¬èªãƒ©ãƒ™ãƒ«ï¼ˆPillowï¼‰ ======\n",
    "def _find_jp_font():\n",
    "    for p in [\n",
    "        \"/System/Library/Fonts/Hiragino Sans W3.ttc\",\n",
    "        \"/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc\",\n",
    "        \"/System/Library/Fonts/Hiragino Sans W6.ttc\",\n",
    "        \"/Library/Fonts/NotoSansCJKjp-Regular.otf\",\n",
    "        \"/Library/Fonts/NotoSansCJK-Regular.ttc\",\n",
    "        \"/Library/Fonts/Arial Unicode.ttf\",\n",
    "    ]:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "_JP_FONT = _find_jp_font()\n",
    "\n",
    "def draw_label_jp(img_rgb, text, xy, font_size=18, pad=4,\n",
    "                  fg=(255, 255, 255), bg=(0, 0, 0)):\n",
    "    font = ImageFont.truetype(_JP_FONT, font_size) if _JP_FONT else ImageFont.load_default()\n",
    "    img_pil = Image.fromarray(img_rgb)\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "    x, y = xy\n",
    "    bbox = draw.textbbox((0, 0), text, font=font)\n",
    "    tw, th = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "    draw.rectangle([x - pad, y - pad, x + tw + pad, y + th + pad], fill=bg)\n",
    "    draw.text((x, y), text, font=font, fill=fg)\n",
    "    return np.array(img_pil)\n",
    "\n",
    "# ====== æ—¢çŸ¥ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆãƒ™ã‚¯ãƒˆãƒ«å˜ä½ã§ãƒ•ãƒ©ãƒƒãƒˆåŒ–ï¼‰ ======\n",
    "def load_known_vectors():\n",
    "    base = Path(\"face_data/people\")\n",
    "    enc_paths = list(base.glob(\"*/encodings.npy\"))\n",
    "    known_encs, owner_by_vec, meta_by_vec = [], [], []\n",
    "    for ep in enc_paths:\n",
    "        person_name = ep.parent.name.split(\"_\", 1)[-1]\n",
    "        meta_obj = {}\n",
    "        mp = ep.parent / \"meta.json\"\n",
    "        if mp.exists():\n",
    "            try:\n",
    "                meta_all = json.loads(mp.read_text(encoding=\"utf-8\"))\n",
    "                meta_obj = meta_all.get(\"meta\", {})\n",
    "            except Exception:\n",
    "                meta_obj = {}\n",
    "        arr = np.load(str(ep))\n",
    "        if arr.ndim == 1 and arr.shape[0] == 128:\n",
    "            arr = arr.reshape(1, 128)\n",
    "        for row in arr:\n",
    "            known_encs.append(row.astype(np.float64))\n",
    "            owner_by_vec.append(person_name)\n",
    "            meta_by_vec.append(meta_obj)\n",
    "    return (np.array(known_encs) if known_encs else None), owner_by_vec, meta_by_vec\n",
    "\n",
    "# ====== Mail.app ã‚’ AppleScript ã§è‡ªå‹•é€ä¿¡ ======\n",
    "def send_via_mail_app(recipient, subject, body, attachments):\n",
    "    tmp_script = Path(\"send_mail.scpt\")\n",
    "    attach_lines = \"\\n\".join(\n",
    "        [f'set end of attchs to (POSIX file \"{str(Path(p).resolve())}\")'\n",
    "         for p in attachments]\n",
    "    )\n",
    "    subject_apple = subject.replace('\"', \"'\")\n",
    "    body_apple = body.replace('\"', \"'\")\n",
    "    script_content = f\"\"\"\n",
    "    on run\n",
    "        set theRecipient to \"{recipient}\"\n",
    "        set theSubject to \"{subject_apple}\"\n",
    "        set theContent to \"{body_apple}\"\n",
    "        set attchs to {{}}\n",
    "{attach_lines}\n",
    "        tell application \"Mail\"\n",
    "            set newMsg to make new outgoing message with properties {{visible:false, subject:theSubject, content:theContent & return & return}}\n",
    "            tell newMsg\n",
    "                make new to recipient at end of to recipients with properties {{address:theRecipient}}\n",
    "                repeat with f in attchs\n",
    "                    try\n",
    "                        make new attachment with properties {{file name:f}} at after the last paragraph\n",
    "                    end try\n",
    "                end repeat\n",
    "                send\n",
    "            end tell\n",
    "        end tell\n",
    "    end run\n",
    "    \"\"\"\n",
    "    tmp_script.write_text(script_content, encoding=\"utf-8\")\n",
    "    subprocess.run([\"osascript\", str(tmp_script)], check=True)\n",
    "    tmp_script.unlink(missing_ok=True)\n",
    "\n",
    "# ====== macOS TTSï¼ˆAIéŸ³å£°ï¼‰ ======\n",
    "def make_tts_audio(text: str) -> Path:\n",
    "    Path(\"share_out\").mkdir(exist_ok=True)\n",
    "    stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_path = Path(f\"share_out/voice_{stamp}.aiff\")\n",
    "    # æ—¥æœ¬èªéŸ³å£°ï¼ˆKyoko ãªã©ï¼‰ã«ã—ãŸã„å ´åˆã¯ -v ã‚ªãƒ—ã‚·ãƒ§ãƒ³æŒ‡å®šå¯: [\"say\",\"-v\",\"Kyoko\",...]\n",
    "    subprocess.run([\"say\", text, \"-o\", str(out_path)], check=True)\n",
    "    return out_path\n",
    "\n",
    "# ====== é¡”æ¤œå‡ºï¼ˆHaar â†’ HOG ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰ ======\n",
    "def haar_boxes(rgb):\n",
    "    haar = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    H, W = rgb.shape[:2]\n",
    "    SCALE = 0.6\n",
    "    small = cv2.resize(rgb, (int(W * SCALE), int(H * SCALE)))\n",
    "    gray = cv2.cvtColor(small, cv2.COLOR_RGB2GRAY)\n",
    "    gray = cv2.equalizeHist(gray)\n",
    "    faces = haar.detectMultiScale(gray, scaleFactor=1.05,\n",
    "                                  minNeighbors=4, minSize=(60, 60))\n",
    "    boxes = []\n",
    "    for (x, y, w, h) in faces:\n",
    "        l = int(x / SCALE)\n",
    "        t = int(y / SCALE)\n",
    "        r = int((x + w) / SCALE)\n",
    "        b = int((y + h) / SCALE)\n",
    "        boxes.append((t, r, b, l))\n",
    "    return boxes\n",
    "\n",
    "def hog_boxes(rgb, up=1):\n",
    "    return face_recognition.face_locations(\n",
    "        rgb, number_of_times_to_upsample=up, model=\"hog\"\n",
    "    )\n",
    "\n",
    "# ====== ãƒ¡ã‚¤ãƒ³ï¼š5ç§’æ’®å½±â†’æœ€ã‚‚æ˜ã‚‹ã„ãƒ•ãƒ¬ãƒ¼ãƒ ã§èªè­˜â†’ãƒ¡ãƒ¼ãƒ«é€ä¿¡ï¼ˆã‚·ãƒ³ãƒ—ãƒ«TTSï¼‰ ======\n",
    "def identify_and_email(capture_time: float = 5.0):\n",
    "    import time\n",
    "\n",
    "    def brightness(img):\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        return gray.mean()\n",
    "\n",
    "    # æ—¢çŸ¥ãƒ‡ãƒ¼ã‚¿\n",
    "    known_encs, owners, metas = load_known_vectors()\n",
    "\n",
    "    # ã‚«ãƒ¡ãƒ©èµ·å‹•\n",
    "    cap = cv2.VideoCapture(CAMERA_INDEX, cv2.CAP_AVFOUNDATION)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, FRAME_WIDTH)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)\n",
    "\n",
    "    # éœ²å‡ºå®‰å®š\n",
    "    for _ in range(6):\n",
    "        cap.read()\n",
    "\n",
    "    # 5ç§’é–“ã§æœ€ã‚‚æ˜ã‚‹ã„ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’é¸ã¶\n",
    "    start = time.time()\n",
    "    best_frame, best_rgb, best_brightness = None, None, -1\n",
    "    while time.time() - start < capture_time:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            continue\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        br = brightness(rgb)\n",
    "        if br > best_brightness:\n",
    "            best_frame, best_rgb, best_brightness = frame.copy(), rgb.copy(), br\n",
    "    cap.release()\n",
    "\n",
    "    if best_rgb is None:\n",
    "        print(\"âŒ ã‚«ãƒ¡ãƒ©ã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ\")\n",
    "        return\n",
    "\n",
    "    # æ¤œå‡ºï¼ˆHaar â†’ HOGï¼‰\n",
    "    boxes = haar_boxes(best_rgb)\n",
    "    detector = \"Haar\"\n",
    "    if not boxes:\n",
    "        boxes = hog_boxes(best_rgb, up=1)\n",
    "        detector = \"HOGx1\"\n",
    "\n",
    "    # å‡ºåŠ›æº–å‚™\n",
    "    vis = best_rgb.copy()\n",
    "    stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_dir = Path(\"share_out\")\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "    attachments = []\n",
    "    report_lines = [\n",
    "        f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        f\"Detections: {len(boxes)}\",\n",
    "        f\"Detector: {detector}\",\n",
    "        \"\"\n",
    "    ]\n",
    "\n",
    "    # èªè­˜ï¼†æç”»ï¼†åˆ‡ã‚Šå‡ºã—\n",
    "    names_for_tts = []\n",
    "    if len(boxes):\n",
    "        encs = face_recognition.face_encodings(best_rgb, known_face_locations=boxes)\n",
    "        for idx, (box, enc) in enumerate(zip(boxes, encs), start=1):\n",
    "            t, r, b, l = box\n",
    "            cv2.rectangle(vis, (l, t), (r, b), (0, 255, 0), 2)\n",
    "            label = \"Unknown\"\n",
    "            meta = {}\n",
    "            best = None\n",
    "\n",
    "            if (known_encs is not None) and len(known_encs):\n",
    "                d = face_recognition.face_distance(known_encs, enc)\n",
    "                i = int(np.argmin(d))\n",
    "                best = float(d[i])\n",
    "                if best <= FACE_TOLERANCE:\n",
    "                    label = owners[i]\n",
    "                    meta = metas[i] if isinstance(metas[i], dict) else {}\n",
    "                    vis = draw_label_jp(\n",
    "                        vis, f\"{label} ({best:.3f})\", (l, max(5, t - 24))\n",
    "                    )\n",
    "                    y0 = b + 6\n",
    "                    for j, (k, v) in enumerate(list(meta.items())[:3]):\n",
    "                        vis = draw_label_jp(\n",
    "                            vis, f\"{k}: {v}\", (l, y0 + j * 22)\n",
    "                        )\n",
    "                else:\n",
    "                    vis = draw_label_jp(\n",
    "                        vis, f\"Unknown ({best:.3f})\", (l, max(5, t - 24))\n",
    "                    )\n",
    "            else:\n",
    "                vis = draw_label_jp(vis, \"Unknown\", (l, max(5, t - 24)))\n",
    "\n",
    "            # æœ¬æ–‡\n",
    "            head = f\"[Face {idx}] {label}\" + (f\"  dist={best:.3f}\" if best is not None else \"\")\n",
    "            report_lines.append(head)\n",
    "            if meta:\n",
    "                for k, v in list(meta.items())[:5]:\n",
    "                    report_lines.append(f\"  - {k}: {v}\")\n",
    "            report_lines.append(\"\")\n",
    "\n",
    "            # TTSç”¨ï¼šåå‰æŠ½å‡ºï¼ˆUnknownã¯é™¤å¤–ï¼‰\n",
    "            if label != \"Unknown\":\n",
    "                names_for_tts.append(label)\n",
    "\n",
    "            # é¡”åˆ‡ã‚Šå‡ºã—ä¿å­˜â†’æ·»ä»˜\n",
    "            crop = best_frame[t:b, l:r]\n",
    "            cp = out_dir / f\"crop_{idx}_{stamp}.jpg\"\n",
    "            cv2.imwrite(str(cp), crop)\n",
    "            attachments.append(cp)\n",
    "    else:\n",
    "        report_lines.append(\"[No face detected]\")\n",
    "\n",
    "    # ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ç”»åƒ\n",
    "    overlay_path = out_dir / f\"snapshot_{stamp}.jpg\"\n",
    "    cv2.imwrite(str(overlay_path), cv2.cvtColor(vis, cv2.COLOR_RGB2BGR))\n",
    "    attachments.insert(0, overlay_path)\n",
    "\n",
    "    # === ã‚·ãƒ³ãƒ—ãƒ«TTSï¼ˆã€Œã€‡ã€‡ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€ã ã‘ï¼‰ ===\n",
    "    try:\n",
    "        if names_for_tts:\n",
    "            tts_text = \"ã€\".join([f\"{n}ã‚’æ¤œå‡ºã—ã¾ã—ãŸ\" for n in names_for_tts])\n",
    "        else:\n",
    "            tts_text = \"é¡”ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ\"\n",
    "        voice_path = make_tts_audio(tts_text)\n",
    "        attachments.append(voice_path)\n",
    "        print(\"ğŸ§ éŸ³å£°ã‚‚æ·»ä»˜ã—ã¾ã—ãŸ:\", voice_path)\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ éŸ³å£°ç”Ÿæˆã«å¤±æ•—:\", e)\n",
    "\n",
    "    # é€ä¿¡ï¼ˆâ˜… ä»¶åã‚’ã€Œã€‡ã€‡ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€å½¢å¼ã«å¤‰æ›´ â˜…ï¼‰\n",
    "    if names_for_tts:\n",
    "        # ä¾‹: ã€ŒçŸ³å²¡å’Œç£¨ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€ or ã€ŒAã‚’æ¤œå‡ºã—ã¾ã—ãŸã€Bã‚’æ¤œå‡ºã—ã¾ã—ãŸã€\n",
    "        subject = \"ã€\".join([f\"{n}ã‚’æ¤œå‡ºã—ã¾ã—ãŸ\" for n in names_for_tts])\n",
    "    else:\n",
    "        subject = \"é¡”ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ\"\n",
    "\n",
    "    body = \"\\n\".join(report_lines)\n",
    "    send_via_mail_app(RECIPIENT, subject, body, attachments)\n",
    "\n",
    "    print(\"âœ… è‡ªå‹•é€ä¿¡å®Œäº†:\", RECIPIENT)\n",
    "    print(\"ä»¶å:\", subject)\n",
    "    print(\"æœ¬æ–‡:\\n\" + body)\n",
    "    print(\"æ·»ä»˜:\")\n",
    "    for p in attachments:\n",
    "        print(\" -\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4613484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_and_email(capture_time=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0500ace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
